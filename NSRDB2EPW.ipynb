{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a590fd6-ea5f-429a-b9da-2d7e4586038b",
   "metadata": {},
   "source": [
    "# NSRDB to EPW Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b098e-a004-4ad7-ab5d-41666db9e5b0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34615c31-aa42-4301-976a-a086e8128669",
   "metadata": {},
   "source": [
    "This notebook allows the downloading of climate data and automtic conversion into EPW files for any year (where available dataset exists on NSRDB) for almost every location in the Americas.\n",
    "\n",
    "### Conceptual steps\n",
    "\n",
    "+ Gather your query location as a [WKT geometry](https://libgeos.org/specifications/wkt/) (in WGS84 CRS, could be Point, Polygon, MultiPolygon, etc. but a minimum working example is a Point) and prepare it as a string\n",
    "+ Determine the dataset you would like to query, and the appropriate temporal resolutions and the years you neeed.\n",
    "+ Obtain an API key. You can [sign up for your API key](https://developer.nrel.gov/signup/).\n",
    "+ Translate the geometry into NSRDB point_ids (automated by the pipeline, no need to worry)\n",
    "+ Get the weather data about the associated point_ids and parse them into DataFrames and write them as CSVs (automated by the pipeline, no need to worry)\n",
    "+ Translate these DataFrames into EPW files and write them (automated by the pipeline, no need to worry)\n",
    "\n",
    "### Credits\n",
    "\n",
    "The dataset and a interactive web portal is available via [NSRDB Data Viewer](https://nsrdb.nrel.gov/data-viewer). This pipeline takes advantage of the sample query code provided here.\n",
    "\n",
    "Thanks to [Patrick's script](https://github.com/building-energy/epw/blob/master/epw/epw.py) we have a ready-made workflow for EPW file generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dded51-39e5-46ae-86b1-d74e70f7fa49",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ad52c-f3a7-4558-be45-27b1819d0eb3",
   "metadata": {},
   "source": [
    "### 1. Prepare your WKT geometry\n",
    "\n",
    "Prepare your WKT geometry representing the area of investigation as a string. Further guidance available [here](https://libgeos.org/specifications/wkt/).\n",
    "\n",
    "A minimum working example is a Point, such as `POINT(-76.48408307172359 42.45094507085529)` is the location of Cornell AAP.\n",
    "\n",
    "### 2. Determine the right temporal resolution and coverage\n",
    "\n",
    "By referring to the table below, determine the right temporal resolution and coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642b8c8-8602-41ac-8e09-c0dca7bf8a4c",
   "metadata": {},
   "source": [
    "Datasets and their coverage:\n",
    "\n",
    "|Geographies|Name|Temporal Resolution|Geographical Resolution|Years (Inclusive)|\n",
    "|------|------|------|------|------|\n",
    "|USA Continental and Mexico|`nsrdb-GOES-conus`|5, 30, 60min|2km|2021-23|\n",
    "|USA and Americas|`nsrdb-GOES-full-disc`|10, 30, 60min|2km|2018-23|\n",
    "|USA and Americas|`nsrdb-GOES-aggregated`|30, 60min|4km|1998-23|\n",
    "|USA and Americas|`nsrdb-GOES-tmy`|60min| |2022-23|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b370894f-b463-4c43-b31e-1df43c42a89d",
   "metadata": {},
   "source": [
    "### 3. Obtain an API key\n",
    "You are suggested to [sign up for your API key](https://developer.nrel.gov/signup/) before working with the script. For lab purposes you can use the key provided (it is my key actually so pay attention to the payload if you are doing batch downloads for larger regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f50f40-2716-4935-86d8-2c559ff6be9f",
   "metadata": {},
   "source": [
    "### 4. Run the script with the inputs\n",
    "\n",
    "Provide the inputs for the script to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbe20a-219c-4be7-a587-2d8375535db4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01c9d401-8bb0-4dfd-8c00-cbaf7d81fc13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95aac3d-43b2-4dea-b1bf-f763b5a6ffb6",
   "metadata": {},
   "source": [
    "## Workflow (No need to uncollapse these cells, just run all of them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4da7ae-298b-4736-a643-5aa0cdb38253",
   "metadata": {},
   "source": [
    "### Get Points by WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64258f3f-6229-4691-bf21-9e800af22634",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = {\n",
    "    'CONUS': 'nsrdb-GOES-conus-v4-0-0',\n",
    "    'full-disc': 'nsrdb-GOES-full-disc-v4-0-0',\n",
    "    'TMY': 'nsrdb-GOES-tmy-v4-0-0',\n",
    "    'aggregated': 'nsrdb-GOES-aggregated-v4-0-0'\n",
    "}\n",
    "def get_points(wkt: str='POINT(-74.25820375161103+42.684861252913805)', dataset=dataset_names['full-disc']):\n",
    "    req_template = 'https://maps-api.nrel.gov/bigdata/v2/sample-code?email=insert.your.email%40fake.com&wkt={}&attributes=dew_point&names=%272023%27,%272021%27&interval=15&to_utc=false&api_key=%7B%7BYOUR_API_KEY%7D%7D&dataset={}'\n",
    "    req = req_template.format(wkt.replace(' ', '+'), dataset)\n",
    "    response = requests.get(req)\n",
    "    script = dict(response.json())['outputs']['script']\n",
    "\n",
    "    pattern = r\"POINTS = \\[(.*?)\\]\"\n",
    "    match = re.search(pattern, script, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"POINTS block not found in the script\")\n",
    "\n",
    "    points_block = match.group(1)\n",
    "    points_list = re.findall(r'\\d+', points_block)\n",
    "    points_list = [int(point) for point in points_list]\n",
    "\n",
    "    return points_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4844d01-7fd6-4eb1-80b3-86d7f516df7c",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3db75f26-13b1-4207-a68f-8c1373e4a0a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    input_data = {\n",
    "        'attributes': 'dew_point,ghi,air_temperature,wind_direction,surface_albedo,dhi,dni,surface_pressure,wind_speed',\n",
    "        'interval': INTERVAL,\n",
    "        'to_utc': 'false',\n",
    "        'api_key': API_KEY,\n",
    "        'email': EMAIL,\n",
    "    }\n",
    "    files_written = []\n",
    "    for name in YEARS:\n",
    "        print(f\"Processing name: {name}\")\n",
    "        for id, location_ids in enumerate(POINTS):\n",
    "            input_data['names'] = [name]\n",
    "            input_data['location_ids'] = location_ids\n",
    "            print(f'Making request for point group {id + 1} of {len(POINTS)}...')\n",
    "\n",
    "            if '.csv' in BASE_URL:\n",
    "                url = BASE_URL + urllib.parse.urlencode(input_data, True)\n",
    "                # Note: CSV format is only supported for single point requests\n",
    "                # Suggest that you might append to a larger data frame\n",
    "                data = pd.read_csv(url)\n",
    "                print(f'Response data (you should replace this print statement with your processing): {data.shape}')\n",
    "                # You can use the following code to write it to a file\n",
    "                data.to_csv(RESULTS_DIR + '{}_{}.csv'.format(location_ids, name))\n",
    "                files_written.append(RESULTS_DIR + '{}_{}.csv'.format(location_ids, name))\n",
    "            else:\n",
    "                headers = {\n",
    "                  'x-api-key': API_KEY\n",
    "                }\n",
    "                data = get_response_json_and_handle_errors(requests.post(BASE_URL, input_data, headers=headers))\n",
    "                download_url = data['outputs']['downloadUrl']\n",
    "                # You can do with what you will the download url\n",
    "                print(data['outputs']['message'])\n",
    "                print(f\"Data can be downloaded from this url when ready: {download_url}\")\n",
    "\n",
    "                # Delay for 1 second to prevent rate limiting\n",
    "                time.sleep(1)\n",
    "            print(f'Processed')\n",
    "    return files_written\n",
    "\n",
    "\n",
    "def get_response_json_and_handle_errors(response: requests.Response) -> dict:\n",
    "    \"\"\"Takes the given response and handles any errors, along with providing\n",
    "    the resulting json\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response : requests.Response\n",
    "        The response object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The resulting json\n",
    "    \"\"\"\n",
    "    if response.status_code != 200:\n",
    "        print(f\"An error has occurred with the server or the request. The request response code/status: {response.status_code} {response.reason}\")\n",
    "        print(f\"The response body: {response.text}\")\n",
    "        exit(1)\n",
    "\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "    except:\n",
    "        print(f\"The response couldn't be parsed as JSON, likely an issue with the server, here is the text: {response.text}\")\n",
    "        exit(1)\n",
    "\n",
    "    if len(response_json['errors']) > 0:\n",
    "        errors = '\\n'.join(response_json['errors'])\n",
    "        print(f\"The request errored out, here are the errors: {errors}\")\n",
    "        exit(1)\n",
    "    return response_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb428e-0245-4a3c-8b32-a7b96cccccb3",
   "metadata": {},
   "source": [
    "### Write to EPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e6c6130-a598-4efb-9071-94d695280434",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class epw():\n",
    "    \"\"\"A class which represents an EnergyPlus weather (epw) file\n",
    "    https://github.com/building-energy/epw/blob/master/epw/epw.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.headers={}\n",
    "        self.dataframe=pd.DataFrame()\n",
    "\n",
    "\n",
    "    def read(self,fp):\n",
    "        \"\"\"Reads an epw file\n",
    "\n",
    "        Arguments:\n",
    "            - fp (str): the file path of the epw file\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.headers=self._read_headers(fp)\n",
    "        self.dataframe=self._read_data(fp)\n",
    "\n",
    "\n",
    "    def _read_headers(self,fp):\n",
    "        \"\"\"Reads the headers of an epw file\n",
    "\n",
    "        Arguments:\n",
    "            - fp (str): the file path of the epw file\n",
    "\n",
    "        Return value:\n",
    "            - d (dict): a dictionary containing the header rows\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        d={}\n",
    "        with open(fp, newline='') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            for row in csvreader:\n",
    "                if row[0].isdigit():\n",
    "                    break\n",
    "                else:\n",
    "                    d[row[0]]=row[1:]\n",
    "        return d\n",
    "\n",
    "\n",
    "    def _read_data(self,fp):\n",
    "        \"\"\"Reads the climate data of an epw file\n",
    "\n",
    "        Arguments:\n",
    "            - fp (str): the file path of the epw file\n",
    "\n",
    "        Return value:\n",
    "            - df (pd.DataFrame): a DataFrame comtaining the climate data\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        names=['Year',\n",
    "               'Month',\n",
    "               'Day',\n",
    "               'Hour',\n",
    "               'Minute',\n",
    "               'Data Source and Uncertainty Flags',\n",
    "               'Dry Bulb Temperature',\n",
    "               'Dew Point Temperature',\n",
    "               'Relative Humidity',\n",
    "               'Atmospheric Station Pressure',\n",
    "               'Extraterrestrial Horizontal Radiation',\n",
    "               'Extraterrestrial Direct Normal Radiation',\n",
    "               'Horizontal Infrared Radiation Intensity',\n",
    "               'Global Horizontal Radiation',\n",
    "               'Direct Normal Radiation',\n",
    "               'Diffuse Horizontal Radiation',\n",
    "               'Global Horizontal Illuminance',\n",
    "               'Direct Normal Illuminance',\n",
    "               'Diffuse Horizontal Illuminance',\n",
    "               'Zenith Luminance',\n",
    "               'Wind Direction',\n",
    "               'Wind Speed',\n",
    "               'Total Sky Cover',\n",
    "               'Opaque Sky Cover (used if Horizontal IR Intensity missing)',\n",
    "               'Visibility',\n",
    "               'Ceiling Height',\n",
    "               'Present Weather Observation',\n",
    "               'Present Weather Codes',\n",
    "               'Precipitable Water',\n",
    "               'Aerosol Optical Depth',\n",
    "               'Snow Depth',\n",
    "               'Days Since Last Snowfall',\n",
    "               'Albedo',\n",
    "               'Liquid Precipitation Depth',\n",
    "               'Liquid Precipitation Quantity']\n",
    "\n",
    "        first_row=self._first_row_with_climate_data(fp)\n",
    "        df=pd.read_csv(fp,\n",
    "                       skiprows=first_row,\n",
    "                       header=None,\n",
    "                       names=names)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def _first_row_with_climate_data(self,fp):\n",
    "        \"\"\"Finds the first row with the climate data of an epw file\n",
    "\n",
    "        Arguments:\n",
    "            - fp (str): the file path of the epw file\n",
    "\n",
    "        Return value:\n",
    "            - i (int): the row number\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        with open(fp, newline='') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            for i,row in enumerate(csvreader):\n",
    "                if row[0].isdigit():\n",
    "                    break\n",
    "        return i\n",
    "\n",
    "\n",
    "    def write(self,fp):\n",
    "        \"\"\"Writes an epw file\n",
    "\n",
    "        Arguments:\n",
    "            - fp (str): the file path of the new epw file\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        with open(fp, 'w', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter=',',\n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            for k,v in self.headers.items():\n",
    "                csvwriter.writerow([k]+v)\n",
    "            for row in self.dataframe.itertuples(index= False):\n",
    "                csvwriter.writerow(i for i in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc4322dd-b992-4403-b51e-2c35d7e02ed3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def relative_humidity(t, dew):\n",
    "    return np.exp(17.62 * dew / (dew + 243.12) - 17.62 * t / (t + 243.12))\n",
    "def CSV2EPW(file):\n",
    "    df = pd.read_csv(file, index_col=0, skiprows=2).reset_index(drop=True)\n",
    "    df = df.drop(columns=df.columns[df.isna().sum() == df.shape[0]])\n",
    "    metadata = pd.read_csv(file, index_col=0, nrows=1).reset_index(drop=True)\n",
    "\n",
    "    year = df['Year'].iloc[0]\n",
    "    interval = str((df['Hour'].iloc[1] - df['Hour'].iloc[0]) * 60 +  df['Minute'].iloc[1] - df['Minute'].iloc[0])\n",
    "    \n",
    "    a = epw()\n",
    "    epw_df = a.dataframe\n",
    "    \n",
    "    timezone, elevation, location_id = metadata['Local Time Zone'].iloc[0], metadata['Elevation'].iloc[0], metadata['Location ID'].iloc[0]\n",
    "    lat, lon = metadata['Latitude'].iloc[0], metadata['Longitude'].iloc[0]\n",
    "\n",
    "    a.headers = {'LOCATION': [LOCATION,\n",
    "      STATE,\n",
    "      COUNTRY,\n",
    "      metadata['Source'].iloc[0],\n",
    "      'XXX',\n",
    "      lat,\n",
    "      lon,\n",
    "      timezone,\n",
    "      elevation],\n",
    "     'DESIGN CONDITIONS': ['1',\n",
    "       'Climate Design Data 2009 ASHRAE Handbook',\n",
    "       '',\n",
    "       'Heating',\n",
    "       '1',\n",
    "       '3.8',\n",
    "       '4.9',\n",
    "       '-3.7',\n",
    "       '2.8',\n",
    "       '10.7',\n",
    "       '-1.2',\n",
    "       '3.4',\n",
    "       '11.2',\n",
    "       '12.9',\n",
    "       '12.1',\n",
    "       '11.6',\n",
    "       '12.2',\n",
    "       '2.2',\n",
    "       '150',\n",
    "       'Cooling',\n",
    "       '8',\n",
    "       '8.5',\n",
    "       '28.3',\n",
    "       '17.2',\n",
    "       '25.7',\n",
    "       '16.7',\n",
    "       '23.6',\n",
    "       '16.2',\n",
    "       '18.6',\n",
    "       '25.7',\n",
    "       '17.8',\n",
    "       '23.9',\n",
    "       '17',\n",
    "       '22.4',\n",
    "       '5.9',\n",
    "       '310',\n",
    "       '16.1',\n",
    "       '11.5',\n",
    "       '19.9',\n",
    "       '15.3',\n",
    "       '10.9',\n",
    "       '19.2',\n",
    "       '14.7',\n",
    "       '10.4',\n",
    "       '18.7',\n",
    "       '52.4',\n",
    "       '25.8',\n",
    "       '49.8',\n",
    "       '23.8',\n",
    "       '47.6',\n",
    "       '22.4',\n",
    "       '2038',\n",
    "       'Extremes',\n",
    "       '12.8',\n",
    "       '11.5',\n",
    "       '10.6',\n",
    "       '22.3',\n",
    "       '1.8',\n",
    "       '34.6',\n",
    "       '1.5',\n",
    "       '2.3',\n",
    "       '0.8',\n",
    "       '36.2',\n",
    "       '-0.1',\n",
    "       '37.5',\n",
    "       '-0.9',\n",
    "       '38.8',\n",
    "       '-1.9',\n",
    "       '40.5'],\n",
    "     'TYPICAL/EXTREME PERIODS': ['6',\n",
    "     'Summer - Week Nearest Max Temperature For Period',\n",
    "     'Extreme',\n",
    "     '8/ 1',\n",
    "     '8/ 7',\n",
    "     'Summer - Week Nearest Average Temperature For Period',\n",
    "     'Typical',\n",
    "     '9/ 5',\n",
    "     '9/11',\n",
    "     'Winter - Week Nearest Min Temperature For Period',\n",
    "     'Extreme',\n",
    "     '2/ 1',\n",
    "     '2/ 7',\n",
    "     'Winter - Week Nearest Average Temperature For Period',\n",
    "     'Typical',\n",
    "     '2/15',\n",
    "     '2/21',\n",
    "     'Autumn - Week Nearest Average Temperature For Period',\n",
    "     'Typical',\n",
    "     '12/ 6',\n",
    "     '12/12',\n",
    "     'Spring - Week Nearest Average Temperature For Period',\n",
    "     'Typical',\n",
    "     '5/29',\n",
    "     '6/ 4'],\n",
    "     'GROUND TEMPERATURES': ['3',\n",
    "     '.5',\n",
    "     '',\n",
    "     '',\n",
    "     '',\n",
    "     '10.86',\n",
    "     '10.57',\n",
    "     '11.08',\n",
    "     '11.88',\n",
    "     '13.97',\n",
    "     '15.58',\n",
    "     '16.67',\n",
    "     '17.00',\n",
    "     '16.44',\n",
    "     '15.19',\n",
    "     '13.51',\n",
    "     '11.96',\n",
    "     '2',\n",
    "     '',\n",
    "     '',\n",
    "     '',\n",
    "     '11.92',\n",
    "     '11.41',\n",
    "     '11.51',\n",
    "     '11.93',\n",
    "     '13.33',\n",
    "     '14.60',\n",
    "     '15.61',\n",
    "     '16.15',\n",
    "     '16.03',\n",
    "     '15.32',\n",
    "     '14.17',\n",
    "     '12.95',\n",
    "     '4',\n",
    "     '',\n",
    "     '',\n",
    "     '',\n",
    "     '12.79',\n",
    "     '12.27',\n",
    "     '12.15',\n",
    "     '12.31',\n",
    "     '13.10',\n",
    "     '13.96',\n",
    "     '14.74',\n",
    "     '15.28',\n",
    "     '15.41',\n",
    "     '15.10',\n",
    "     '14.42',\n",
    "     '13.60'],\n",
    "     'HOLIDAYS/DAYLIGHT SAVINGS': ['No', '0', '0', '0'],\n",
    "     'COMMENTS 1': [metadata['Source'].iloc[0]],\n",
    "     'COMMENTS 2': ['https://es.aap.cornell.edu/', 'https://github.com/kastnerp/NREL-PSB3-2-EPW'],\n",
    "     'DATA PERIODS': ['1', '1', 'Data', 'Sunday', ' 1/ 1', '12/31']}\n",
    "\n",
    "    dt = pd.date_range('01/01/' + str(year), periods=8760, freq='h')\n",
    "    missing_values = np.array(np.ones(8760) * 999999).astype(int)\n",
    "    \n",
    "    epw_df['Year'] = dt.year.astype(int)\n",
    "    epw_df['Month'] = dt.month.astype(int)\n",
    "    epw_df['Day'] = dt.day.astype(int)\n",
    "    epw_df['Hour'] = dt.hour.astype(int) + 1\n",
    "    epw_df['Minute'] = dt.minute.astype(int)\n",
    "    epw_df['Data Source and Uncertainty Flags'] = missing_values\n",
    "    \n",
    "    epw_df['Dry Bulb Temperature'] = df['Temperature'].values.flatten()\n",
    "    \n",
    "    epw_df['Dew Point Temperature'] = df['Dew Point'].values.flatten()\n",
    "    \n",
    "    epw_df['Relative Humidity'] = df.apply(lambda x: relative_humidity(x['Temperature'], x['Dew Point']),axis=1).apply(lambda x: int(np.round(x * 100))).values.flatten() # changes\n",
    "    \n",
    "    epw_df['Atmospheric Station Pressure'] = (df['Pressure']*100).values.flatten()\n",
    "    epw_df['Extraterrestrial Horizontal Radiation'] = missing_values\n",
    "    #\n",
    "    epw_df['Extraterrestrial Direct Normal Radiation'] = missing_values\n",
    "    #\n",
    "    epw_df['Horizontal Infrared Radiation Intensity'] = missing_values\n",
    "    #\n",
    "    epw_df['Global Horizontal Radiation'] = df['GHI'].values.flatten()\n",
    "    epw_df['Direct Normal Radiation'] = df['DNI'].values.flatten()\n",
    "    epw_df['Diffuse Horizontal Radiation'] = df['DHI'].values.flatten()\n",
    "    \n",
    "    epw_df['Global Horizontal Illuminance'] = missing_values\n",
    "    epw_df['Direct Normal Illuminance'] = missing_values\n",
    "    epw_df['Diffuse Horizontal Illuminance'] = missing_values\n",
    "    epw_df['Zenith Luminance'] = missing_values\n",
    "    \n",
    "    epw_df['Wind Direction'] = df['Wind Direction'].values.flatten().astype(int)\n",
    "    epw_df['Wind Speed'] = df['Wind Speed'].values.flatten()\n",
    "    \n",
    "    epw_df['Total Sky Cover'] = missing_values # df['Cloud Type'].values.flatten() # changes\n",
    "    # used if Horizontal IR Intensity missing\n",
    "    epw_df['Opaque Sky Cover'] = missing_values # df['Cloud Type'].values.flatten() # changes\n",
    "    #\n",
    "    \n",
    "    epw_df['Visibility'] = missing_values\n",
    "    epw_df['Ceiling Height'] = missing_values\n",
    "    epw_df['Present Weather Observation'] = missing_values\n",
    "    #\n",
    "    epw_df['Present Weather Codes'] = missing_values\n",
    "    epw_df['Precipitable Water'] = missing_values # df['Precipitable Water'].values.flatten() # changes\n",
    "    epw_df['Aerosol Optical Depth'] = missing_values\n",
    "    #\n",
    "    epw_df['Snow Depth'] = missing_values\n",
    "    epw_df['Days Since Last Snowfall'] = missing_values\n",
    "    epw_df['Albedo'] = df['Surface Albedo'].values.flatten()\n",
    "    #\n",
    "    \n",
    "    epw_df['Liquid Precipitation Depth'] = missing_values\n",
    "    epw_df['Liquid Precipitation Quantity'] = missing_values\n",
    "    \n",
    "    a.dataframe = epw_df\n",
    "    \n",
    "    d = \"_\"\n",
    "    \n",
    "    file_name = RESULTS_DIR + str(LOCATION) + d + str(lat) + d + str(lon) + d + str(year) + '.epw'\n",
    "    a.write(file_name)\n",
    "    print('Successfully written {}'.format(file_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e404a5-5e98-4927-b3ff-53e8bd691914",
   "metadata": {},
   "source": [
    "## API KEY\n",
    "\n",
    "You can [sign up for your API key](https://developer.nrel.gov/signup/) to use it this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "906fb75d-8fda-4655-a108-18cca66d90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'VR0y2pOyC6BMFt1I6gkFMipFc1o4ixgWUbnEhkPH' # use your own key if possible\n",
    "#with open('archive/key.txt', mode='r', encoding='utf-8') as f:\n",
    "#    API_KEY = f.read().strip()\n",
    "assert API_KEY != ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52740701-1771-46f2-af42-ab08349bd4ef",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057cec9-b407-4b8e-8b41-68109647a25a",
   "metadata": {},
   "source": [
    "### Non essential metadata - put in right information for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28fc1ae4-c1af-46b3-a574-11dd31985f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non critial metadata\n",
    "LOCATION = 'Ithaca' # just naming\n",
    "STATE = 'STATE' # just naming\n",
    "COUNTRY = 'United States' # just naming\n",
    "EMAIL = \"cl2749@cornell.edu\" # your email, does not really matter if you are downloading csv directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfac5dd-cf72-44eb-9bfa-4ddd9c351c5b",
   "metadata": {},
   "source": [
    "### Critical inputs - these will determine the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed504ad7-3e83-400b-90a1-47154e0f19a0",
   "metadata": {},
   "source": [
    "+ `WKT` is the WKT (Well Known Text) representation of the location you are downloading the EPW files. Technically all WKT geometries are accepted, including points, polygons, and multipolygons. **But if you are not familiar with this concept, simply input the lat-long point of the city/town you are working on**. For example `POINT(-76.48408307172359 42.45094507085529)` (**no comma!**) is for the location of Cornell AAP.\n",
    "+ `DATASET` is the full name of the dataset you are downloading from. Choose one from the dictionary `dataset_names` above.\n",
    "+ `INTERVAL` is the temporal resolution. Pay attention to what data is available by referring to the table above. Note that this field is always a **string** type!\n",
    "+ `YEARS` is the list of years to download data. Pay attention to what data is available by referring to the table above. Note that this field is always a list of **string**s.\n",
    "+ `RESULTS_DIR` is the folder location to save the downloaded files. Include the dash symbols `/`. For example: `my_location/` is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7b03d02-329f-42f1-8fc1-694e2b212e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = {\n",
    "    'CONUS': 'nsrdb-GOES-conus-v4-0-0',\n",
    "    'full-disc': 'nsrdb-GOES-full-disc-v4-0-0',\n",
    "    'TMY': 'nsrdb-GOES-tmy-v4-0-0',\n",
    "    'aggregated': 'nsrdb-GOES-aggregated-v4-0-0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9dc38bb7-904f-458a-bd27-749cfe0d28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lat, Long representation of the location, as a minimum working example.\n",
    "# Example: 'POINT(-76.48408307172359 42.45094507085529)' is the location of Cornell AAP\n",
    "WKT = 'POINT(-74.25820375161103 42.684861252913805)'\n",
    "\n",
    "DATASET = dataset_names['full-disc'] # see dataset_names and table above. example: dataset_names['full-disc']\n",
    "INTERVAL = '60' # temporal resolution, example: '60'\n",
    "YEARS = ['2020', '2021', '2022', '2023'] # example: ['2020', '2021', '2022', '2023']\n",
    "RESULTS_DIR='results/' # example: 'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974bd71-46c0-496e-b857-3b169ae92282",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd0f5186-547f-415a-91b4-e7231ba818fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-existent directory. New directory created at: results/\n",
      "Processing name: 2020\n",
      "Making request for point group 1 of 1...\n",
      "Response data (you should replace this print statement with your processing): (8762, 20)\n",
      "Processed\n",
      "Processing name: 2021\n",
      "Making request for point group 1 of 1...\n",
      "Response data (you should replace this print statement with your processing): (8762, 20)\n",
      "Processed\n",
      "Processing name: 2022\n",
      "Making request for point group 1 of 1...\n",
      "Response data (you should replace this print statement with your processing): (8762, 20)\n",
      "Processed\n",
      "Processing name: 2023\n",
      "Making request for point group 1 of 1...\n",
      "Response data (you should replace this print statement with your processing): (8762, 20)\n",
      "Processed\n",
      "Successfully written results/Ithaca_42.68_-74.25_2020.epw\n",
      "Successfully written results/Ithaca_42.68_-74.25_2021.epw\n",
      "Successfully written results/Ithaca_42.68_-74.25_2022.epw\n",
      "Successfully written results/Ithaca_42.68_-74.25_2023.epw\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "    print(f\"Non-existent directory. New directory created at: {RESULTS_DIR}\")\n",
    "\n",
    "POINTS = get_points(wkt=WKT, dataset=dataset_names['full-disc'])\n",
    "BASE_URL = \"https://developer.nrel.gov/api/nsrdb/v2/solar/{}-download.csv?\".format(DATASET)\n",
    "assert len(POINTS) > 0\n",
    "files_written = download_data()\n",
    "for file in files_written:\n",
    "    CSV2EPW(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e2ee4-4abf-4603-90c0-bcf9336577f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
